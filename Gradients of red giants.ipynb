{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automated differentiation - gradients of red giant stars \n",
    "\n",
    "By using Autograd and refactoring most of my Keras code into vectorized numpy, I am able to have a high degree of leverage in observing what the neural network learns. Here, I investigate results, gradients and mathematical justification for what the neural networks learns and how it fits into the astrophysics picture. We have three (two) main models to analyze:\n",
    "\n",
    "1. A classification model assuming the confidence of a RGB star is approximately logistic\n",
    "2. A regression model for $\\Pi_1$ of a red giant, the period spacing\n",
    "3. A regression model for $\\Delta{}v$, the large frequency seperation of a red giant\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "\\text{Model 1} &= p(x|W, b) = \\sigma(r(XW_1 + b_1)W_2 + b_2) &&\\text{Architecture: } W_1 \\in (7514, 32), b_1 \\in (32,), W_2 \\in (32, 1), b_2 \\in (1,)\\\\\n",
    "\\text{Model 2} &= \\hat{y}_{\\Pi_1} = (r(r(XW_1 + b_1)W_2 + b_2)W_3 + b_3)W_4 + b_4 &&\\text{Architecture: } W_1 \\in (7514, 8), b_1 \\in (8,), W_2 \\in (8, 4), b_2 \\in (4,), W_3 \\in (4, 2) b_3 \\in (2,), W_4 \\in (2, 1), b_4 \\in (1,)\\\\\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Where $x$ is random variable of stellar spectra to the set $\\{0, 1\\}$ that is Bernoulli distributed with probability $p = p(x)$. $r$ is the ReLU function, $\\max(0, x)$, and $\\sigma$ is the sigmoid function, $\\frac{1}{1+e^{-z}}$. The reason for the relatively thin layers in the regression for period spacing is to avoid overfitting and also because there didn't seem to be a huge cost in loss on the validation set, even when we botteleneck the last layer to a vector in $\\mathbb{R}^2$. This is actually good for forcing the network to learn a feature rich vector and also is easy to visualize. \n",
    "\n",
    "With this explicit equation, we can derive the gradients easily - in this case, we use `Autograd` to perform automatic differentation on `numpy` operations in Python to obtain rich mathematical results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Gradients of red giants - classification\n",
    "\n",
    "Here, we take the gradient of the prediction with respect to the input vector of red giants and average the gradient to get an average picture of what features in the stellar spectra influence the greatest change in the prediction, explicitly we are plotting\n",
    "\n",
    "$$\\text{Average gradient of red giants} = \\frac{1}{N}\\sum_{i=1}^N\\nabla_{\\hat{p}}(x^{(i)})$$\n",
    "\n",
    "![title](plots/gradient/fig1.png)\n",
    "\n",
    "From this plot we can see immediately that the areas of highest magnitude are $(5444, 5585), (6850, 7080)$. Here we observe them in wavelength space and look at the corresponding ions. Below is the gradient with the x-axis in the open interval $(6850, 7080)$ converted to wavelength space $(16740, 16840)$\n",
    "\n",
    "![title](plots/gradient/fig2.png)\n",
    "\n",
    "This interval of wavelength mostly corresponds to Fe, Iron, specifically Fe I. However, the noticable dip downwards at around $16820.8$ corresponds to the ion Th I - Thorium. Since the output of $\\hat{y}$ is the probability that a star is a red clump, this indicates a increase in Thorium leads to the lower probability that a star is a red clump. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Gradients of red giants - $\\Pi_1$ regression\n",
    "\n",
    "Here we take the gradient of the regression on $Pi_1$ of a star and normalize it, averaging the gradient of all the training examples in the entire Kepler data set. Specifically plotted below is:\n",
    "\n",
    "$$\\text{Average gradient of red giants} = \\frac{1}{N}\\sum_{i=1}^N\\nabla_{\\hat{y}}(x^{(i)})$$\n",
    "\n",
    "![title](plots/gradient/fig4.png)\n",
    "\n",
    "Some sections in the gradient that are especially noisy and high in magnitude, as well as unusually tall are $(0, 980), (1843, 3558)$. Below is the gradient plotted in wavelength space for $(0, 980)$\n",
    "\n",
    "![title](plots/gradient/fig5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient comparisons\n",
    "![title](plots/gradient/fig3.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
